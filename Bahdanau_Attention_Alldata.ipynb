{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DN_YC_lj0gx8","outputId":"b9972ea1-cdc6-47ed-809b-00dfc2a602ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting textattack\n","  Downloading textattack-0.3.8-py3-none-any.whl (418 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n","  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n","Collecting flair (from textattack)\n","  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.12.0)\n","Collecting language-tool-python (from textattack)\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Collecting lemminflect (from textattack)\n","  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lru-dict (from textattack)\n","  Downloading lru_dict-1.1.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n","Collecting datasets==2.4.0 (from textattack)\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.22.4)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.10.1)\n","Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0.0+cu118)\n","Collecting transformers>=4.21.0 (from textattack)\n","  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting terminaltables (from textattack)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.65.0)\n","Collecting word2number (from textattack)\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting num2words (from textattack)\n","  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (9.1.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n","Collecting pinyin==0.4.0 (from textattack)\n","  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n","Collecting OpenHowNet (from textattack)\n","  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n","Collecting pycld2 (from textattack)\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting click<8.1.0 (from textattack)\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (9.0.0)\n","Collecting dill<0.3.6 (from datasets==2.4.0->textattack)\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (2.27.1)\n","Collecting xxhash (from datasets==2.4.0->textattack)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==2.4.0->textattack)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (2023.4.0)\n","Collecting aiohttp (from datasets==2.4.0->textattack)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0 (from datasets==2.4.0->textattack)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (23.1)\n","Collecting responses<0.19 (from datasets==2.4.0->textattack)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2022.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.7.0->textattack) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.7.0->textattack) (16.0.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.21.0->textattack)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.1)\n","Collecting segtok>=1.5.7 (from flair->textattack)\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting mpld3==0.3 (from flair->textattack)\n","  Downloading mpld3-0.3.tar.gz (788 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n","Collecting sqlitedict>=1.6.0 (from flair->textattack)\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting deprecated>=1.2.4 (from flair->textattack)\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.7)\n","Collecting boto3 (from flair->textattack)\n","  Downloading boto3-1.26.133-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n","  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.8.10)\n","Collecting langdetect (from flair->textattack)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.2)\n","Collecting ftfy (from flair->textattack)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting janome (from flair->textattack)\n","  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gdown==4.4.0 (from flair->textattack)\n","  Downloading gdown-4.4.0.tar.gz (14 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting conllu>=4.0 (from flair->textattack)\n","  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n","Collecting wikipedia-api (from flair->textattack)\n","  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n","Collecting pptree (from flair->textattack)\n","  Downloading pptree-3.1.tar.gz (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-revgrad (from flair->textattack)\n","  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n","Collecting transformer-smaller-training-vocab>=0.2.1 (from flair->textattack)\n","  Downloading transformer_smaller_training_vocab-0.2.3-py3-none-any.whl (12 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack) (1.16.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack) (4.11.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.2.0)\n","Collecting docopt>=0.6.2 (from num2words->textattack)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting anytree (from OpenHowNet->textattack)\n","  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n","Collecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.4.0->textattack)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.4.0->textattack)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.4.0->textattack)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.4.0->textattack)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.4.0->textattack)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.8.0->flair->textattack) (6.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.18.3)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.10.9.7)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (3.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.1.0)\n","Collecting protobuf<=3.20.2 (from transformers>=4.21.0->textattack)\n","  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.133 (from boto3->flair->textattack)\n","  Downloading botocore-1.29.133-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->flair->textattack)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->flair->textattack)\n","  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair->textattack) (0.2.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.2)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.4.0->textattack)\n","  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n","Collecting accelerate>=0.19.0 (from transformers>=4.21.0->textattack)\n","  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack) (2.4.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->transformers>=4.21.0->textattack) (5.9.5)\n","Building wheels for collected packages: pinyin, gdown, mpld3, pycld2, word2number, docopt, sqlitedict, langdetect, pptree\n","  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=bbbdc61b2ff4b2a7dbaaf461b5b8c3723d5cf887b4e44a093d494862106198ce\n","  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n","  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=25aba1bc98379affd2beab3e6e0b247a48b4346c1a3d7590d6bf32b0edacce23\n","  Stored in directory: /root/.cache/pip/wheels/03/0b/3f/6ddf67a417a5b400b213b0bb772a50276c199a386b12c06bfc\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116685 sha256=40c05e71919f050ee6dde5a26ddfc777c6ac94f767213430bc65193c98218e0c\n","  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9915758 sha256=38d5e728cf540253ff63e29e3b2b1d8151e088bfc2bf8d037770e3c4c35529ed\n","  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5569 sha256=997b69cc29713bedfd35dffd8a61ae46a4fa6a265cae00df9a1d80bf18a56aa2\n","  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=3900673bd31f51181ef39d4ff2bdb5fda07a35cd21267eab07e5785b4a49a570\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=81ee68dfe2557b2965382b641df851c8b1789fccac6bc76ad7ec2a74109d050a\n","  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=7e290347de8ff0e6625d5743c43730a52dc227347a1df201f323d29257c2f5fb\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=5b1ddf27f24bca29544dda34932e9febd681dbeb780f4ac56a1c3c3e0ab227f1\n","  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n","Successfully built pinyin gdown mpld3 pycld2 word2number docopt sqlitedict langdetect pptree\n","Installing collected packages: word2number, tokenizers, sqlitedict, sentencepiece, pycld2, pptree, pinyin, mpld3, lru-dict, janome, docopt, xxhash, terminaltables, segtok, protobuf, num2words, multidict, lemminflect, langdetect, jmespath, ftfy, frozenlist, dill, deprecated, conllu, click, async-timeout, anytree, yarl, wikipedia-api, responses, OpenHowNet, multiprocess, language-tool-python, huggingface-hub, botocore, aiosignal, transformers, s3transfer, gdown, bpemb, aiohttp, boto3, datasets, accelerate, transformer-smaller-training-vocab, pytorch-revgrad, flair, bert-score, textattack\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.3\n","    Uninstalling click-8.1.3:\n","      Successfully uninstalled click-8.1.3\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 4.6.6\n","    Uninstalling gdown-4.6.6:\n","      Successfully uninstalled gdown-4.6.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed OpenHowNet-2.0 accelerate-0.19.0 aiohttp-3.8.4 aiosignal-1.3.1 anytree-2.8.0 async-timeout-4.0.2 bert-score-0.3.13 boto3-1.26.133 botocore-1.29.133 bpemb-0.3.4 click-8.0.4 conllu-4.5.2 datasets-2.4.0 deprecated-1.2.13 dill-0.3.5.1 docopt-0.6.2 flair-0.12.2 frozenlist-1.3.3 ftfy-6.1.1 gdown-4.4.0 huggingface-hub-0.14.1 janome-0.4.2 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.1.8 mpld3-0.3 multidict-6.0.4 multiprocess-0.70.13 num2words-0.5.12 pinyin-0.4.0 pptree-3.1 protobuf-3.20.2 pycld2-0.41 pytorch-revgrad-0.2.0 responses-0.18.0 s3transfer-0.6.1 segtok-1.5.11 sentencepiece-0.1.99 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.8 tokenizers-0.13.3 transformer-smaller-training-vocab-0.2.3 transformers-4.29.1 wikipedia-api-0.5.8 word2number-1.1 xxhash-3.2.0 yarl-1.9.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pygobject in /usr/lib/python3/dist-packages (3.36.0)\n","Collecting pycairo>=1.11.1 (from pygobject)\n","  Downloading pycairo-1.23.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.6/344.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pycairo\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycairo \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for pycairo (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for pycairo\u001b[0m\u001b[31m\n","\u001b[0mFailed to build pycairo\n","\u001b[31mERROR: Could not build wheels for pycairo, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gTTS\n","  Downloading gTTS-2.3.2-py3-none-any.whl (28 kB)\n","Collecting pyttsx3\n","  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n","Collecting playsound\n","  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.27.1)\n","Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4)\n","Building wheels for collected packages: playsound\n","  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7019 sha256=238786e352feebcbf10ee8c94700e82fffc3d79a46e0dd8c8ce95c054dc85467\n","  Stored in directory: /root/.cache/pip/wheels/90/89/ed/2d643f4226fc8c7c9156fc28abd8051e2d2c0de37ae51ac45c\n","Successfully built playsound\n","Installing collected packages: pyttsx3, playsound, gTTS\n","Successfully installed gTTS-2.3.2 playsound-1.3.0 pyttsx3-2.90\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting python-vlc\n","  Downloading python_vlc-3.0.18122-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-vlc\n","Successfully installed python-vlc-3.0.18122\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SpeechRecognition\n","  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n","Installing collected packages: pydub, SpeechRecognition\n","Successfully installed SpeechRecognition-3.10.0 pydub-0.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: smart_open in /usr/local/lib/python3.10/dist-packages (6.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting attention\n","  Downloading attention-5.0.0-py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from attention) (1.22.4)\n","Requirement already satisfied: tensorflow>=2.1 in /usr/local/lib/python3.10/dist-packages (from attention) (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (23.3.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (1.54.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (0.4.8)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (23.1)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow>=2.1->attention)\n","  Downloading protobuf-4.23.0-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (2.12.2)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1->attention) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1->attention) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.1->attention) (0.1.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.1->attention) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (2.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.1->attention) (3.2.2)\n","Installing collected packages: protobuf, attention\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.2\n","    Uninstalling protobuf-3.20.2:\n","      Successfully uninstalled protobuf-3.20.2\n","Successfully installed attention-5.0.0 protobuf-4.23.0\n"]}],"source":["!pip install textattack\n","!pip3 install pygobject\n","!pip3 install gTTS pyttsx3 playsound\n","!pip3 install python-vlc\n","!pip3 install SpeechRecognition pydub\n","!pip install --upgrade smart_open\n","!pip install attention"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmXVpT7m0ix5","outputId":"96d0539f-ec96-4d07-cb75-d8c869fec05e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import re\n","import base64\n","import requests\n","from sys import argv, stdin\n","import os\n","import yaml\n","from gensim.models import Word2Vec\n","from nltk.corpus import stopwords\n","\n","import pickle # save model\n","path = '/content/drive/My Drive/ColabNotebooks/298B_Data_Add_New/'\n","\n","from gtts import gTTS\n","import IPython\n","import speech_recognition as sr\n","from pydub import AudioSegment\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras import layers, activations, models, preprocessing, utils\n","from keras.utils import pad_sequences\n","from keras.layers import Input,Embedding,Bidirectional,LSTM,Dense,Concatenate\n","from keras.models import Model\n","tf.keras.utils.set_random_seed(1234)\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_style(\"whitegrid\")\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","import csv\n","import random\n","import unicodedata\n","import codecs\n","import itertools\n","from __future__ import absolute_import, division, print_function, unicode_literals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdDgkCwNKN40"},"outputs":[],"source":["import csv\n","import random\n","import unicodedata\n","import codecs\n","import itertools\n","from __future__ import absolute_import, division, print_function, unicode_literals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PV3GluBcQAzG"},"outputs":[],"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","from keras.utils import plot_model\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"mUDL4kk922i5"},"source":["Barista data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Lal0ITwgZ8e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjjrRN8nQA06","outputId":"7b838f5d-d118-43b5-9928-c23010a6b50b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# barista file path\n","path_1 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task1-greetings-trn.txt'\n","path_2 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task2-order-drink-no-greetings-trn.txt'\n","path_3 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task3-order-drink-no-greetings-changes-trn.txt'\n","path_4 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task4-order-all-no-greetings-trn.txt'\n","path_5 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task5-order-all-no-greetings-changes-trn.txt'\n","path_6 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task6-order-all-with-greetings-trn.txt'\n","path_7 = '/content/drive/My Drive/ColabNotebooks/data/Barista/barista-task7-order-all-with-greetings-changes-trn.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2p_OM5guVRT9"},"outputs":[],"source":["def parse_candidates(filename):\n","    with open(filename, 'r') as f:\n","        return [' '.join(line.strip().split(' ')[1:]) for line in f]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcOHarZPVRY1"},"outputs":[],"source":["text_1 = parse_candidates(path_1)\n","text_2 = parse_candidates(path_2)\n","text_3 = parse_candidates(path_3)\n","text_4 = parse_candidates(path_4)\n","text_5 = parse_candidates(path_5)\n","text_6 = parse_candidates(path_6)\n","text_7 = parse_candidates(path_7)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYaQdC-PVRdg","outputId":"ad8e9115-7e6c-4f51-9d3b-63244f381817"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hey !\\tHello ! Which drink can I get for you ?',\n"," 'That is correct .\\tGreat ! Can I take your name ? api_call getCustomerName',\n"," 'Keri Russell .\\tYour order will be ready at the next counter , Keri .',\n"," 'Thank you !\\tHave a nice day !',\n"," '',\n"," 'Hello !\\tHello ! Which drink can I get for you ?',\n"," 'Yeah .\\tGreat ! Can I take your name ? api_call getCustomerName',\n"," 'Kevin Costner .\\tYour order will be ready at the next counter , Kevin .',\n"," 'Thank you !\\tHave a nice day !',\n"," '']"]},"metadata":{},"execution_count":8}],"source":["text_1[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVg67nybVWxe"},"outputs":[],"source":["def parse_dialogs(filename):\n","    q = []\n","    a = []\n","\n","    for line in filename:\n","        if line.strip() == '':\n","            continue\n","        else:\n","            splitted = line.strip().split('\\t')\n","            if len(splitted) == 2:\n","                user_utt, bot_utt = splitted\n","                q.append(user_utt)\n","                a.append(bot_utt)\n","            else:\n","                continue\n","\n","    return q, a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKX6rOJBVWz-"},"outputs":[],"source":["q1, a1 = parse_dialogs(text_1)\n","q2, a2 = parse_dialogs(text_2)\n","q3, a3 = parse_dialogs(text_3)\n","q4, a4 = parse_dialogs(text_4)\n","q5, a5 = parse_dialogs(text_5)\n","q6, a6 = parse_dialogs(text_6)\n","q7, a7 = parse_dialogs(text_7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTSexEboVacs"},"outputs":[],"source":["questions = q1+q2+q3+q4+q5+q6+q7\n","answers = a1+a2+a3+a4+a5+a6+a7"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSmy-hTCVafC","outputId":"a8d92ced-3942-404a-cf2c-3087c1cb5083"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hey !',\n"," 'That is correct .',\n"," 'Keri Russell .',\n"," 'Thank you !',\n"," 'Hello !',\n"," 'Yeah .',\n"," 'Kevin Costner .',\n"," 'Thank you !',\n"," 'Hi !',\n"," 'Yeap .']"]},"metadata":{},"execution_count":12}],"source":["questions[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T784kWZeVahJ","outputId":"546052ea-3b5f-49a8-846c-3e4c1732194f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello ! Which drink can I get for you ?',\n"," 'Great ! Can I take your name ? api_call getCustomerName',\n"," 'Your order will be ready at the next counter , Keri .',\n"," 'Have a nice day !',\n"," 'Hello ! Which drink can I get for you ?',\n"," 'Great ! Can I take your name ? api_call getCustomerName',\n"," 'Your order will be ready at the next counter , Kevin .',\n"," 'Have a nice day !',\n"," 'Hello ! Which drink can I get for you ?',\n"," 'Great ! Can I take your name ? api_call getCustomerName']"]},"metadata":{},"execution_count":13}],"source":["answers[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pf3eHlz3VW1z","outputId":"d3bd7361-0a1e-4415-fc60-a59b7a6e3810"},"outputs":[{"output_type":"stream","name":"stdout","text":["36265 36265\n"]}],"source":["print(len(questions), len(answers))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEEVT0J_UJOF","outputId":"a768883f-6d86-42d4-b9f3-6cbb96865829"},"outputs":[{"output_type":"stream","name":"stdout","text":["Question:  Hey !\n","Answer:  Hello ! Which drink can I get for you ?\n"]}],"source":["print(\"Question: \", questions[0])\n","print(\"Answer: \", answers[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmPcTkmizYxj","outputId":"913463b2-4b4f-4ac2-b8c2-214954cc022f"},"outputs":[{"output_type":"stream","name":"stdout","text":["                       Question  \\\n","5000  I would like a cordusio .   \n","5001          Medium would do .   \n","5002              No , thanks .   \n","5003                     Yeah .   \n","5004      A caramel macchiato .   \n","5005                Small cup .   \n","5006         That would be it .   \n","\n","                                                 Answer  \n","5000                                       Which size ?  \n","5001           Would you like anything else with that ?  \n","5002  So that would be a medium cordusio . Is that r...  \n","5003     Your order will be ready at the next counter .  \n","5004                                       Which size ?  \n","5005           Would you like anything else with that ?  \n","5006  So that would be a small caramel macchiato . I...  \n"]}],"source":["import pandas as pd\n","\n","barista = {'Question': questions, 'Answer': answers}\n","barista_df = pd.DataFrame(barista)\n","\n","print(barista_df.iloc[5000:5007,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"2yHfSQBMzfuk","outputId":"0dae0710-6c15-465d-b3ff-0a7e89cb93e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 Question  \\\n","0                                   Hey !   \n","1                       That is correct .   \n","2                          Keri Russell .   \n","3                             Thank you !   \n","4                                 Hello !   \n","...                                   ...   \n","36260                   Medium would do .   \n","36261  I would like a blueberry oatmeal .   \n","36262                       Sounds good !   \n","36263                 It is Johnny Depp .   \n","36264                               Bye !   \n","\n","                                                  Answer  \n","0                Hello ! Which drink can I get for you ?  \n","1      Great ! Can I take your name ? api_call getCus...  \n","2      Your order will be ready at the next counter ,...  \n","3                                      Have a nice day !  \n","4                Hello ! Which drink can I get for you ?  \n","...                                                  ...  \n","36260           Would you like anything else with that ?  \n","36261  So that would be a medium americano and a blue...  \n","36262  Great ! Can I take your name ? api_call getCus...  \n","36263  Your order will be ready at the next counter ,...  \n","36264                                  Have a nice day !  \n","\n","[36265 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-a4413480-2604-4d48-a8dd-f110ab0750b1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hey !</td>\n","      <td>Hello ! Which drink can I get for you ?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>That is correct .</td>\n","      <td>Great ! Can I take your name ? api_call getCus...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Keri Russell .</td>\n","      <td>Your order will be ready at the next counter ,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Thank you !</td>\n","      <td>Have a nice day !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hello !</td>\n","      <td>Hello ! Which drink can I get for you ?</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36260</th>\n","      <td>Medium would do .</td>\n","      <td>Would you like anything else with that ?</td>\n","    </tr>\n","    <tr>\n","      <th>36261</th>\n","      <td>I would like a blueberry oatmeal .</td>\n","      <td>So that would be a medium americano and a blue...</td>\n","    </tr>\n","    <tr>\n","      <th>36262</th>\n","      <td>Sounds good !</td>\n","      <td>Great ! Can I take your name ? api_call getCus...</td>\n","    </tr>\n","    <tr>\n","      <th>36263</th>\n","      <td>It is Johnny Depp .</td>\n","      <td>Your order will be ready at the next counter ,...</td>\n","    </tr>\n","    <tr>\n","      <th>36264</th>\n","      <td>Bye !</td>\n","      <td>Have a nice day !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>36265 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4413480-2604-4d48-a8dd-f110ab0750b1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a4413480-2604-4d48-a8dd-f110ab0750b1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a4413480-2604-4d48-a8dd-f110ab0750b1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}],"source":["barista_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4JNahsXztwz","outputId":"90b28ade-d1da-4a64-9f7d-91f94a270036"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# text preprocessing - lowercase + remove char not english or numbers\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","\n","stop_words = set(stopwords.words('english'))\n","contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n","                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n","                \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n","                \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \n","                \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n","                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n","                \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n","                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n","                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n","                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n","                \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n","                \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n","                \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n","                \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \n","                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n","                \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n","                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n","                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n","                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","#remove_stop = True for questions\n","#remove_stop = False for answers\n","def preprocess_text(text,remove_stop):\n","    text = text.lower()\n","    text = ' '.join([contractions[word] if word in contractions else word for word in text.split()])\n","    text = re.sub(r'[^a-zA-Z0-9]',' ',text)\n","    if remove_stop == True:\n","        text = ' '.join([word for word in text.split() if word not in stop_words])\n","    text = ' '.join([word for word in text.split()])\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"ZYqxyV_F17Sg","outputId":"03e0c6fe-828f-43dd-f6ba-ac79233784a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                               Question  \\\n","0                                   hey   \n","1                       that is correct   \n","2                          keri russell   \n","3                             thank you   \n","4                                 hello   \n","...                                 ...   \n","36260                   medium would do   \n","36261  i would like a blueberry oatmeal   \n","36262                       sounds good   \n","36263                 it is johnny depp   \n","36264                               bye   \n","\n","                                                  Answer  \n","0                    hello which drink can i get for you  \n","1      great can i take your name api call getcustome...  \n","2      your order will be ready at the next counter keri  \n","3                                        have a nice day  \n","4                    hello which drink can i get for you  \n","...                                                  ...  \n","36260             would you like anything else with that  \n","36261  so that would be a medium americano and a blue...  \n","36262  great can i take your name api call getcustome...  \n","36263  your order will be ready at the next counter j...  \n","36264                                    have a nice day  \n","\n","[36265 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-71306c78-3198-401d-9ef1-9457de7e2b9f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hey</td>\n","      <td>hello which drink can i get for you</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>that is correct</td>\n","      <td>great can i take your name api call getcustome...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>keri russell</td>\n","      <td>your order will be ready at the next counter keri</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>thank you</td>\n","      <td>have a nice day</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hello</td>\n","      <td>hello which drink can i get for you</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>36260</th>\n","      <td>medium would do</td>\n","      <td>would you like anything else with that</td>\n","    </tr>\n","    <tr>\n","      <th>36261</th>\n","      <td>i would like a blueberry oatmeal</td>\n","      <td>so that would be a medium americano and a blue...</td>\n","    </tr>\n","    <tr>\n","      <th>36262</th>\n","      <td>sounds good</td>\n","      <td>great can i take your name api call getcustome...</td>\n","    </tr>\n","    <tr>\n","      <th>36263</th>\n","      <td>it is johnny depp</td>\n","      <td>your order will be ready at the next counter j...</td>\n","    </tr>\n","    <tr>\n","      <th>36264</th>\n","      <td>bye</td>\n","      <td>have a nice day</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>36265 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71306c78-3198-401d-9ef1-9457de7e2b9f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-71306c78-3198-401d-9ef1-9457de7e2b9f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-71306c78-3198-401d-9ef1-9457de7e2b9f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["barista_df.Question = barista_df.Question.apply(preprocess_text,remove_stop=False)\n","barista_df.Answer = barista_df.Answer.apply(preprocess_text,remove_stop=False)\n","barista_df"]},{"cell_type":"markdown","metadata":{"id":"SDHOEW832xy9"},"source":["Movie data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhKHQrUDiJ-r"},"outputs":[],"source":["Mlines = open('/content/drive/My Drive/ColabNotebooks/data/OtherConversation/movie_lines.txt',encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n","Mconversations = open('/content/drive/My Drive/ColabNotebooks/data/OtherConversation/movie_conversations.txt',encoding = 'utf-8', errors = 'ignore').read().split('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYyduuDSjmBY"},"outputs":[],"source":["# splits each line of the file into a dictionary of fields(lineID,characterID,movieID,character,text)\n","line_fields = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n","lines = {}\n","\n","with open('/content/drive/My Drive/ColabNotebooks/data/OtherConversation/movie_lines.txt', 'r', encoding='iso-8859-1') as f:\n","    for line in f:\n","        values = line.split(\" +++$+++ \")\n","        # Extract fields\n","        lineObj = {}\n","        for i, field in enumerate(line_fields):\n","            lineObj[field] = values[i]\n","        lines[lineObj['lineID']] = lineObj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8q7KkCy4kJKd"},"outputs":[],"source":["# Grouping fields of lines from the above loaded lines into conversation based on \"movie_conversations.txt\"\n","\n","conv_fields = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n","conversations = []\n","\n","with open('/content/drive/My Drive/ColabNotebooks/data/OtherConversation/movie_conversations.txt', 'r', encoding='iso-8859-1') as f:\n","    for line in f:\n","        values = line.split(\" +++$+++ \")\n","        # Extract fields\n","        convObj = {}\n","        for i, field in enumerate(conv_fields):\n","            convObj[field] = values[i]\n","        lineIds = eval(convObj[\"utteranceIDs\"])\n","        # Reassemble lines\n","        convObj[\"lines\"] = []\n","        for lineId in lineIds:\n","            convObj[\"lines\"].append(lines[lineId])\n","        conversations.append(convObj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qwZA1hjkagc"},"outputs":[],"source":["# Extracts pairs of sentences from conversations\n","qa_pairs = []\n","for conversation in conversations:\n","    # Iterate over all the lines of the conversation\n","    for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n","        inputLine = conversation[\"lines\"][i][\"text\"].strip()\n","        targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n","  \n","        # Filter wrong samples (if one of the lists is empty)\n","        if inputLine and targetLine:\n","            qa_pairs.append([inputLine, targetLine])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E43sK8XP-fEz","outputId":"a2a22721-6955-4160-ae33-d66bb1803a77"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Writing newly formatted file...\n"]}],"source":["# Define path to new file\n","datafile = os.path.join('/content/drive/My Drive/ColabNotebooks/data/OtherConversation', \"formatted_movie_lines.txt\")\n","\n","delimiter = '\\t'\n","# Unescape the delimiter\n","delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n","\n","# Writing the conversational pairs into new csv file\n","print(\"\\nWriting newly formatted file...\")\n","with open(datafile, 'w', encoding='utf-8') as outputfile:\n","    writer = csv.writer(outputfile, delimiter=delimiter)\n","    for pair in qa_pairs:\n","        writer.writerow(pair)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FOnauMRtl-Jq","outputId":"1595807e-0d17-4282-d8c9-9e5e1ca31011"},"outputs":[{"output_type":"stream","name":"stdout","text":["b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\"\n","b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\"\n","b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"\n","b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\"\n","b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\"\n","b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"\n","b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\"\n","b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.'\n","b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\"\n","b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.'\n"]}],"source":["# visualizing some lines\n","with open(datafile,'rb') as file:\n","    lines = file.readlines()\n","for line in lines[:10]:\n","    print(line.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNtUBSO4-fKv"},"outputs":[],"source":["# Default word tokens\n","PAD_token = 0  # Used for padding short sentences\n","SOS_token = 1  # Start-of-sentence token\n","EOS_token = 2  # End-of-sentence token\n","\n","class Vocabulary:\n","    def __init__(self, name):\n","        self.name = name\n","        self.trimmed = False\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3  # Counting SOS, EOS, PAD\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.num_words\n","            self.word2count[word] = 1\n","            self.index2word[self.num_words] = word\n","            self.num_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    # Removing words below a certain count threshold\n","    def trim(self, min_count):\n","        if self.trimmed:\n","            return\n","        self.trimmed = True\n","\n","        keep_words = []\n","\n","        for k, v in self.word2count.items():\n","            if v >= min_count:\n","                keep_words.append(k)\n","\n","        print('keep_words {} / {} = {:.4f}'.format(\n","            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n","        ))\n","\n","        # Reinitializing dictionaries\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3 # Counting default tokens\n","\n","        for word in keep_words:\n","            self.addWord(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":42},"id":"iDfCbMVO-6ds","outputId":"3a906c89-aa9a-455a-9ecf-715482123ae7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cedillea'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["# Turn a Unicode string to plain ASCII\n","def unicodeToAscii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","# Testing the function - eliminates special characters\n","unicodeToAscii(\"cédillea\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":42},"id":"8lRcYAgo_AgX","outputId":"d3b6d28f-7e4b-4dbc-d0b7-a163f1d65f87"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'abc ! ? . abcde abc ?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}],"source":["# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    \n","    # Unicode string to plain ASCII\n","    s = unicodeToAscii(s.lower().strip())\n","    \n","    # Replacing any .!? by a whitespace plus the character\n","    # ' \\1' means the first bracketed group\n","    # r is not to consider ' \\1' as an individual character\n","    # r in r\" \\1\" is to esccape the backslash\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    \n","    # Removing any character that is not a sequence of lower or upper case letters\n","    # + means one or more\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    \n","    # Removing a sequence of whitespace characters\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    \n","    return s\n","\n","# Testing the function\n","normalizeString(\"abc1103!?.     abcde' abc?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRniU5So_IlT","outputId":"4faf5ffa-314d-4530-8b69-a18623b0c250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading and processing file....Please Wait\n","Done Reading!!!\n"]}],"source":["datafile = os.path.join('/content/drive/My Drive/ColabNotebooks/data/OtherConversation', \"formatted_movie_lines.txt\")\n","\n","# Reading the file and splitting into lines\n","print(\"Reading and processing file....Please Wait\")\n","lines = open(datafile,encoding = 'utf-8').read().strip().split('\\n')\n","\n","# Splitting every line into pairs and normalizing them\n","pairs = [[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n","\n","print(\"Done Reading!!!\")\n","voc = Vocabulary('/content/drive/My Drive/ColabNotebooks/data/OtherConversation')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cdj4lwJAmHI","outputId":"434b0e7a-77cf-449a-f968-178558dee138"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 221282 pairs/conversations in the dataset\n","After filtering there are 64271 pairs/conversations\n","Counted words: 18008\n","['there .', 'where ?']\n","['you have my word . as a gentleman', 'you re sweet .']\n","['hi .', 'looks like things worked out tonight huh ?']\n","['you know chastity ?', 'i believe we share an art instructor']\n","['have fun tonight ?', 'tons']\n","['well no . . .', 'then that s all you had to say .']\n","['then that s all you had to say .', 'but']\n","['but', 'you always been this selfish ?']\n","['do you listen to this crap ?', 'what crap ?']\n","['what good stuff ?', 'the real you .']\n"]}],"source":["MAX_LENGTH = 10  # Maximum sentence length to consider\n","\n","# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n","def filterPair(p):\n","    # Input sequences need to preserve the last word for EOS token\n","    if p:\n","        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","    else:\n","        return False\n","\n","# Filter pairs using filterPair condition\n","def filterPairs(pairs):\n","    pairs = [pair for pair in pairs if pair != ['']]\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","print(\"There are {} pairs/conversations in the dataset\".format(len(pairs)))\n","pairs = filterPairs(pairs)\n","print(\"After filtering there are {} pairs/conversations\".format(len(pairs)))\n","\n","for pair in pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","\n","print(\"Counted words:\", voc.num_words)\n","for pair in pairs[:10]:\n","    print(pair)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNH75lDRFvTR","outputId":"178c3c70-311f-40be-8037-c6c956ecd64e"},"outputs":[{"output_type":"stream","name":"stdout","text":["keep_words 7823 / 18005 = 0.4345\n","Trimmed from 64271 pairs to 53165, 0.8272 of total\n"]},{"output_type":"execute_result","data":{"text/plain":["[['there .', 'where ?'],\n"," ['you have my word . as a gentleman', 'you re sweet .'],\n"," ['hi .', 'looks like things worked out tonight huh ?'],\n"," ['have fun tonight ?', 'tons'],\n"," ['well no . . .', 'then that s all you had to say .'],\n"," ['then that s all you had to say .', 'but'],\n"," ['but', 'you always been this selfish ?'],\n"," ['do you listen to this crap ?', 'what crap ?'],\n"," ['what good stuff ?', 'the real you .'],\n"," ['wow', 'let s go .']]"]},"metadata":{},"execution_count":31}],"source":["MIN_COUNT = 3    # Minimum word count threshold for trimming\n","\n","def trimRareWords(voc, pairs, MIN_COUNT):\n","    # Trim words used under the MIN_COUNT from the voc\n","    voc.trim(MIN_COUNT)\n","    # Filter out pairs with trimmed words\n","    keep_pairs = []\n","    for pair in pairs:\n","        input_sentence = pair[0]\n","        output_sentence = pair[1]\n","        keep_input = True\n","        keep_output = True\n","        # Check input sentence\n","        for word in input_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_input = False\n","                break\n","        # Check output sentence\n","        for word in output_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_output = False\n","                break\n","\n","        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","\n","    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n","    return keep_pairs\n","\n","\n","# Trim voc and pairs\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)\n","\n","pairs[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"jNM1_O7YMAWB","outputId":"c705fefb-3a0c-4711-be66-fc0bffd78e44"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                              Question  \\\n","0                                there   \n","1      you have my word as a gentleman   \n","2                                   hi   \n","3                     have fun tonight   \n","4                              well no   \n","...                                ...   \n","53160              three minutes to go   \n","53161    another fifteen seconds to go   \n","53162              yes sir name please   \n","53163                             food   \n","53164        do you have a reservation   \n","\n","                                         Answer  \n","0                                         where  \n","1                                  you re sweet  \n","2      looks like things worked out tonight huh  \n","3                                          tons  \n","4                then that s all you had to say  \n","...                                         ...  \n","53160                                       yes  \n","53161                   do something stall them  \n","53162                                      food  \n","53163                 do you have a reservation  \n","53164                                      food  \n","\n","[53165 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-2e987d75-606a-489f-bd9d-d6656b3f55c8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>there</td>\n","      <td>where</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>you have my word as a gentleman</td>\n","      <td>you re sweet</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hi</td>\n","      <td>looks like things worked out tonight huh</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>have fun tonight</td>\n","      <td>tons</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>well no</td>\n","      <td>then that s all you had to say</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>53160</th>\n","      <td>three minutes to go</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>53161</th>\n","      <td>another fifteen seconds to go</td>\n","      <td>do something stall them</td>\n","    </tr>\n","    <tr>\n","      <th>53162</th>\n","      <td>yes sir name please</td>\n","      <td>food</td>\n","    </tr>\n","    <tr>\n","      <th>53163</th>\n","      <td>food</td>\n","      <td>do you have a reservation</td>\n","    </tr>\n","    <tr>\n","      <th>53164</th>\n","      <td>do you have a reservation</td>\n","      <td>food</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>53165 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e987d75-606a-489f-bd9d-d6656b3f55c8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2e987d75-606a-489f-bd9d-d6656b3f55c8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2e987d75-606a-489f-bd9d-d6656b3f55c8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":32}],"source":["# consider the conversations as Q&A to line up with the other datasets\n","movie_questions = []\n","movie_answers = []\n","\n","for pair in pairs:\n","    input_sentence = pair[0]\n","    output_sentence = pair[1]\n","\n","    movie_questions.append(input_sentence)\n","    movie_answers.append(output_sentence)\n","\n","movielines_df = pd.DataFrame()\n","movielines_df['Question'] = movie_questions\n","movielines_df['Answer'] = movie_answers\n","\n","movielines_df.Question = movielines_df.Question.apply(preprocess_text,remove_stop=False)\n","movielines_df.Answer = movielines_df.Answer.apply(preprocess_text,remove_stop=False)\n","\n","movielines_df"]},{"cell_type":"markdown","metadata":{"id":"A3NPwCT3MMib"},"source":["Knowledge data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzIhdRyfCPqf"},"outputs":[],"source":["# knowledge file path\n","knowledge_path = '/content/drive/My Drive/ColabNotebooks/data/OtherConversation/drinking_knowledge_data_200.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJVotCAMCmYt","outputId":"3a6dccc1-d951-4ea9-ae9d-74e426a72611"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 \tWhat is the recommended daily water intake?\n","•\tThe recommended daily water intake is around 2 to 3 liters or 8 to 12 cups.\n","2\tIs drinking water while eating bad for you?\n","•\tNo, drinking water while eating is not bad for you. It can actually aid digestion.\n","3\tCan drinking too much water be harmful?\n","•\tYes, drinking too much water can be harmful and lead to a condition called water intoxication.\n","4\tWhat is the recommended daily caffeine intake?\n","•\tThe recommended daily caffeine intake is no more than 400mg per day, which is equivalent to about 4 cups of coffee.\n","5\tDoes coffee dehydrate you?\n","•\tNo, coffee does not dehydrate you. It is a mild diuretic, but the amount of water in a cup of coffee is enough to offset any fluid loss.\n"]}],"source":["# glance at the data\n","with open('/content/drive/My Drive/ColabNotebooks/data/OtherConversation/drinking_knowledge_data_200.txt',encoding = 'utf-8', errors = 'ignore') as file:\n","    lines = file.readlines()\n","for line in lines[:10]:\n","    print(line.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2r4Ukm_TCtnY"},"outputs":[],"source":["q, a = [], []    \n","\n","with open('/content/drive/My Drive/ColabNotebooks/data/OtherConversation/drinking_knowledge_data_200.txt',encoding = 'utf-8', errors = 'ignore') as file:\n","    line_num = 0\n","    lines = file.readlines()\n","\n","    for line in lines:\n","        line = line.strip().split('\\t')[1]\n","\n","        if line_num % 2 == 0:\n","            q.append(line)\n","        else:\n","            a.append(line)\n","        line_num += 1\n","\n","knowledge_questions, knowledge_answers = q, a"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2t1eEGlOND9A","outputId":"e67ddd62-c705-46b0-b340-61c0eb17196d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample question: What is a blood alcohol test?\n","Sample answer: A blood alcohol test is a laboratory test that measures the amount of alcohol in a person's bloodstream\n"]}],"source":["print(f\"Sample question: {knowledge_questions[20]}\")\n","print(f\"Sample answer: {knowledge_answers[20]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":578},"id":"aGLaaff-NHVI","outputId":"c95de505-b191-49ab-e88c-69084da07f1e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["                                          Question  \\\n","0       what is the recommended daily water intake   \n","1       is drinking water while eating bad for you   \n","2           can drinking too much water be harmful   \n","3    what is the recommended daily caffeine intake   \n","4                        does coffee dehydrate you   \n","..                                             ...   \n","196                        what is a white russian   \n","197                        what is a black russian   \n","198                          what is a grasshopper   \n","199                         what is a whiskey sour   \n","200                              what is a rob roy   \n","\n","                                                Answer  \n","0    the recommended daily water intake is around 2...  \n","1    no drinking water while eating is not bad for ...  \n","2    yes drinking too much water can be harmful and...  \n","3    the recommended daily caffeine intake is no mo...  \n","4    no coffee does not dehydrate you it is a mild ...  \n","..                                                 ...  \n","196  a white russian is a cocktail typically made w...  \n","197  a black russian is a cocktail typically made w...  \n","198  a grasshopper is a cocktail typically made wit...  \n","199  a whiskey sour is a cocktail typically made wi...  \n","200  a rob roy is a cocktail typically made with sc...  \n","\n","[201 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-faa44bd9-bca1-4c86-b951-3317a98934b5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>what is the recommended daily water intake</td>\n","      <td>the recommended daily water intake is around 2...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>is drinking water while eating bad for you</td>\n","      <td>no drinking water while eating is not bad for ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>can drinking too much water be harmful</td>\n","      <td>yes drinking too much water can be harmful and...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>what is the recommended daily caffeine intake</td>\n","      <td>the recommended daily caffeine intake is no mo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>does coffee dehydrate you</td>\n","      <td>no coffee does not dehydrate you it is a mild ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>what is a white russian</td>\n","      <td>a white russian is a cocktail typically made w...</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>what is a black russian</td>\n","      <td>a black russian is a cocktail typically made w...</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>what is a grasshopper</td>\n","      <td>a grasshopper is a cocktail typically made wit...</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>what is a whiskey sour</td>\n","      <td>a whiskey sour is a cocktail typically made wi...</td>\n","    </tr>\n","    <tr>\n","      <th>200</th>\n","      <td>what is a rob roy</td>\n","      <td>a rob roy is a cocktail typically made with sc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>201 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faa44bd9-bca1-4c86-b951-3317a98934b5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-faa44bd9-bca1-4c86-b951-3317a98934b5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-faa44bd9-bca1-4c86-b951-3317a98934b5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":37}],"source":["# used for Barista\n","# text preprocessing - lowercase + remove char not english or numbers\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n","                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n","                \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n","                \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \n","                \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n","                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n","                \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n","                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n","                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n","                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n","                \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n","                \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n","                \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n","                \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \n","                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n","                \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n","                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n","                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n","                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","#remove_stop = True for questions\n","#remove_stop = False for answers\n","def preprocess_text(text,remove_stop):\n","    text = text.lower()\n","    text = ' '.join([contractions[word] if word in contractions else word for word in text.split()])\n","    text = re.sub(r'[^a-zA-Z0-9]',' ',text)\n","    if remove_stop == True:\n","        text = ' '.join([word for word in text.split() if word not in stop_words])\n","    text = ' '.join([word for word in text.split()])\n","    return text\n","\n","# similar, new implemented for knowledge (not using this version for now)\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip()\n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\"\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    # removing contractions\n","    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n","    sentence = re.sub(r\"he's\", \"he is\", sentence)\n","    sentence = re.sub(r\"she's\", \"she is\", sentence)\n","    sentence = re.sub(r\"it's\", \"it is\", sentence)\n","    sentence = re.sub(r\"that's\", \"that is\", sentence)\n","    sentence = re.sub(r\"what's\", \"what is\", sentence)\n","    sentence = re.sub(r\"where's\", \"where is\", sentence)\n","    sentence = re.sub(r\"how's\", \"how is\", sentence)\n","    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n","    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n","    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n","    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n","    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n","    sentence = re.sub(r\"won't\", \"will not\", sentence)\n","    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n","    sentence = re.sub(r\"n't\", \" not\", sentence)\n","    sentence = re.sub(r\"n'\", \"ng\", sentence)\n","    sentence = re.sub(r\"'bout\", \"about\", sentence)\n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n","    sentence = sentence.strip()\n","    return sentence\n","\n","knowledge_df = pd.DataFrame()\n","knowledge_df['Question'] = knowledge_questions\n","knowledge_df['Answer'] = knowledge_answers\n","\n","knowledge_df.Question = knowledge_df.Question.apply(preprocess_text, remove_stop=False)\n","knowledge_df.Answer = knowledge_df.Answer.apply(preprocess_text, remove_stop=False)\n","\n","knowledge_df"]},{"cell_type":"markdown","metadata":{"id":"XAh772hINOhb"},"source":["Combine all data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"55QWyrQANRb7","outputId":"b7dc89ec-95f6-462d-ca19-abf5e4ac96d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                            Question  \\\n","0                                hey   \n","1                    that is correct   \n","2                       keri russell   \n","3                          thank you   \n","4                              hello   \n","...                              ...   \n","53160            three minutes to go   \n","53161  another fifteen seconds to go   \n","53162            yes sir name please   \n","53163                           food   \n","53164      do you have a reservation   \n","\n","                                                  Answer  \n","0                    hello which drink can i get for you  \n","1      great can i take your name api call getcustome...  \n","2      your order will be ready at the next counter keri  \n","3                                        have a nice day  \n","4                    hello which drink can i get for you  \n","...                                                  ...  \n","53160                                                yes  \n","53161                            do something stall them  \n","53162                                               food  \n","53163                          do you have a reservation  \n","53164                                               food  \n","\n","[89430 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-e06e1cbf-c9ac-4a19-8994-f8beacda5313\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hey</td>\n","      <td>hello which drink can i get for you</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>that is correct</td>\n","      <td>great can i take your name api call getcustome...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>keri russell</td>\n","      <td>your order will be ready at the next counter keri</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>thank you</td>\n","      <td>have a nice day</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hello</td>\n","      <td>hello which drink can i get for you</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>53160</th>\n","      <td>three minutes to go</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>53161</th>\n","      <td>another fifteen seconds to go</td>\n","      <td>do something stall them</td>\n","    </tr>\n","    <tr>\n","      <th>53162</th>\n","      <td>yes sir name please</td>\n","      <td>food</td>\n","    </tr>\n","    <tr>\n","      <th>53163</th>\n","      <td>food</td>\n","      <td>do you have a reservation</td>\n","    </tr>\n","    <tr>\n","      <th>53164</th>\n","      <td>do you have a reservation</td>\n","      <td>food</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>89430 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e06e1cbf-c9ac-4a19-8994-f8beacda5313')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e06e1cbf-c9ac-4a19-8994-f8beacda5313 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e06e1cbf-c9ac-4a19-8994-f8beacda5313');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}],"source":["spikebot_df = pd.concat([barista_df, movielines_df], axis=0)\n","spikebot_df # combine barista and movie first, check the sample"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"BkNIgdZRNMTy","outputId":"dd10b96d-a619-4b68-aef8-09476cec5717"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    Question  \\\n","0                        hey   \n","1            that is correct   \n","2               keri russell   \n","3                  thank you   \n","4                      hello   \n","..                       ...   \n","196  what is a white russian   \n","197  what is a black russian   \n","198    what is a grasshopper   \n","199   what is a whiskey sour   \n","200        what is a rob roy   \n","\n","                                                Answer  \n","0                  hello which drink can i get for you  \n","1    great can i take your name api call getcustome...  \n","2    your order will be ready at the next counter keri  \n","3                                      have a nice day  \n","4                  hello which drink can i get for you  \n","..                                                 ...  \n","196  a white russian is a cocktail typically made w...  \n","197  a black russian is a cocktail typically made w...  \n","198  a grasshopper is a cocktail typically made wit...  \n","199  a whiskey sour is a cocktail typically made wi...  \n","200  a rob roy is a cocktail typically made with sc...  \n","\n","[89631 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-df92e337-9be6-4b58-82f0-645bc66fb84e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hey</td>\n","      <td>hello which drink can i get for you</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>that is correct</td>\n","      <td>great can i take your name api call getcustome...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>keri russell</td>\n","      <td>your order will be ready at the next counter keri</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>thank you</td>\n","      <td>have a nice day</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hello</td>\n","      <td>hello which drink can i get for you</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>what is a white russian</td>\n","      <td>a white russian is a cocktail typically made w...</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>what is a black russian</td>\n","      <td>a black russian is a cocktail typically made w...</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>what is a grasshopper</td>\n","      <td>a grasshopper is a cocktail typically made wit...</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>what is a whiskey sour</td>\n","      <td>a whiskey sour is a cocktail typically made wi...</td>\n","    </tr>\n","    <tr>\n","      <th>200</th>\n","      <td>what is a rob roy</td>\n","      <td>a rob roy is a cocktail typically made with sc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>89631 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df92e337-9be6-4b58-82f0-645bc66fb84e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-df92e337-9be6-4b58-82f0-645bc66fb84e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-df92e337-9be6-4b58-82f0-645bc66fb84e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":39}],"source":["spikebot = pd.concat([spikebot_df, knowledge_df], axis=0)\n","spikebot # also add knowledge data, check the sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVsBCSFwNgF9"},"outputs":[],"source":["# save the df to a csv file\n","#from google.colab import drive\n","#drive.mount('/content/drive', force_remount=True)\n","#path = '/content/drive/My Drive/ColabNotebooks/data/OtherConversation/'\n","\n","#spikebot.to_csv(path + 'modeling_data.csv', encoding='utf-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VmydI3zNrck","outputId":"4b557b4d-2f4e-4731-e271-d13020f4991e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# experiment\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","path = '/content/drive/My Drive/ColabNotebooks/data/OtherConversation/'\n","\n","# sample 70% of the data to reduce runtime and RAM use\n","spikebot=spikebot.iloc[:-45000]\n","spikebot.to_csv(path + 'modeling_data_70.csv', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{"id":"C5fK8V1iN9pq"},"source":["Modeling Data Pre, Train/Test Split, Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNOLHNRcN7Ce","outputId":"dd8247e4-9ea2-4edc-b673-38b570775071"},"outputs":[{"output_type":"stream","name":"stdout","text":["44628 44628\n"]}],"source":["#df = spikebot\n","#df = pd.read_csv(path + 'modeling_data.csv')\n","df = pd.read_csv(path + 'modeling_data_70.csv')\n","df = df[df['Question'].notna()]\n","\n","questions = list(df.Question)\n","answers = list(df.Answer)\n","\n","print(len(questions), len(answers))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkpWr37HPMSd","outputId":"c0583317-83c3-46ee-e4cb-573b29d9c15a"},"outputs":[{"output_type":"stream","name":"stdout","text":["44628 44628\n"]}],"source":["#experiment\n","\n","df = spikebot\n","df = pd.read_csv(path + 'modeling_data_70.csv')\n","df = df[df['Question'].notna()]\n","\n","questions = list(df.Question.astype(str))\n","answers = list(df.Answer.astype(str))\n","\n","print(len(questions), len(answers))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abk7VCYFREBQ"},"outputs":[],"source":["def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    w = w.strip()\n","\n","    w = '<start> ' + w + ' <end>'\n","    return w"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJwl9bfbRJTO","outputId":"12424aa3-2524-4c01-e629-c1fb5bbffe7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["<start> hey <end>\n","<start> hello which drink can i get for you <end>\n"]}],"source":["print(preprocess_sentence(questions[0]))\n","print(preprocess_sentence(answers[0]))\n","\n","pre_questions = [preprocess_sentence(w) for w in questions]\n","pre_answers = [preprocess_sentence(w) for w in answers]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd6_nAFGRVYq"},"outputs":[],"source":["def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","    return tensor, lang_tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKr6KZu6RaD1"},"outputs":[],"source":["def load_dataset(data, num_examples=None):\n","    # creating cleaned input, output pairs\n","    if(num_examples != None):\n","        targ_lang, inp_lang, = data[:num_examples]\n","    else:\n","        targ_lang, inp_lang, = data\n","\n","    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","\n","    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MFUY2juRlIj"},"outputs":[],"source":["#num_examples = 30000\n","data = pre_answers, pre_questions\n","tr_enc_text, tr_dec_text, ts_enc_text, ts_dec_text = load_dataset(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8-bJ9AeRsJW","outputId":"47747d5d-5773-471b-b04b-2cc4e6aeb042"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[  1,  28,   2, ...,   0,   0,   0],\n","        [  1,   7,   8, ...,   0,   0,   0],\n","        [  1, 272, 273, ...,   0,   0,   0],\n","        ...,\n","        [  1,  17,   3, ...,   0,   0,   0],\n","        [  1,  84,   2, ...,   0,   0,   0],\n","        [  1,  30, 113, ...,   0,   0,   0]], dtype=int32),\n"," array([[   1,   34,    9, ...,    0,    0,    0],\n","        [   1,   33,   21, ...,    0,    0,    0],\n","        [   1,    8,   19, ...,    0,    0,    0],\n","        ...,\n","        [   1, 1432,    5, ...,    0,    0,    0],\n","        [   1,  340, 1633, ...,    0,    0,    0],\n","        [   1,   40,   13, ...,    0,    0,    0]], dtype=int32),\n"," <keras.preprocessing.text.Tokenizer at 0x7f71640d8f70>,\n"," <keras.preprocessing.text.Tokenizer at 0x7f717da3cee0>)"]},"metadata":{},"execution_count":49}],"source":["tr_enc_text, tr_dec_text, ts_enc_text, ts_dec_text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhCchK2BW0F6","outputId":"11fc8c43-6b87-40c0-cfd0-9bc4d339ffdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["44628 44628\n"]}],"source":["print(len(tr_enc_text),len(tr_dec_text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVlkn-EtINcg"},"outputs":[],"source":["def sents2sequences(tokenizer, sentences, reverse=False, pad_length=None, padding_type='post'):\n","    encoded_text = tokenizer.texts_to_sequences(sentences)\n","    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n","    if reverse:\n","        preproc_text = np.flip(preproc_text, axis=1)\n","    return preproc_text\n","\n","\n","def preprocess_data(enc_tokenizer, dec_tokenizer, enc_text, dec_text):\n","    enc_seq = enc_tokenizer.texts_to_sequences(tr_enc_text)\n","    enc_timesteps = np.max([len(l) for l in enc_seq])\n","    enc_seq = pad_sequences(enc_seq, padding='post', maxlen = enc_timesteps)\n","    dec_seq = dec_tokenizer.texts_to_sequences(tr_dec_text)\n","    dec_timesteps = np.max([len(l) for l in dec_seq])\n","    dec_seq = pad_sequences(dec_seq, padding='post', maxlen = dec_timesteps)\n","    return enc_seq, dec_seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlOO_1j9IP1V"},"outputs":[],"source":["def define_nmt(hidden_size, batch_size, enc_timesteps, enc_vsize, dec_timesteps, dec_vsize):\n","    \"\"\" Defining a NMT model \"\"\"\n","\n","    # Define an input sequence and process it.\n","    if batch_size:\n","        encoder_inputs = Input(batch_shape=(batch_size, enc_timesteps, enc_vsize), name='encoder_inputs')\n","        decoder_inputs = Input(batch_shape=(batch_size, dec_timesteps - 1, dec_vsize), name='decoder_inputs')\n","    else:\n","        encoder_inputs = Input(shape=(enc_timesteps, enc_vsize), name='encoder_inputs')\n","        if fr_timesteps:\n","            decoder_inputs = Input(shape=(dec_timesteps - 1, dec_vsize), name='decoder_inputs')\n","        else:\n","            decoder_inputs = Input(shape=(None, dec_vsize), name='decoder_inputs')\n","\n","    # Encoder GRU\n","    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n","    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n","\n","    # Set up the decoder GRU, using `encoder_states` as initial state.\n","    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n","    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n","\n","    # Attention layer\n","    # attn_layer = AttentionLayer(name='attention_layer')\n","    attn_layer = AdditiveAttention(name=\"attention_layer\")\n","\n","    ## The input for AdditiveAttention: query, key\n","    ## It returns a tensor of shape as query\n","    ## This is different from the AttentionLayer developed by Thushan\n","    # attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n","\n","    attn_out, attn_states  = attn_layer([decoder_out,encoder_out],return_attention_scores=True)\n","\n","    # Concat attention input and decoder GRU output\n","    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n","\n","    # Dense layer\n","    dense = Dense(dec_vsize, activation='softmax', name='softmax_layer')\n","    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n","    decoder_pred = dense_time(decoder_concat_input)\n","\n","    # Full model\n","    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n","    full_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    full_model.summary()\n","\n","    \"\"\" Inference model \"\"\"\n","    batch_size = 1\n","\n","    \"\"\" Encoder (Inference) model \"\"\"\n","    encoder_inf_inputs = Input(batch_shape=(batch_size, enc_timesteps, enc_vsize), name='encoder_inf_inputs')\n","    encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n","    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n","\n","    \"\"\" Decoder (Inference) model \"\"\"\n","    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, dec_vsize), name='decoder_word_inputs')\n","    encoder_inf_states = Input(batch_shape=(batch_size, enc_timesteps, hidden_size), name='encoder_inf_states')\n","    decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_init')\n","\n","    decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n","    # attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n","    attn_inf_out, attn_inf_states  = attn_layer([decoder_inf_out, encoder_inf_states],return_attention_scores=True)\n","\n","    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n","    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n","    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n","                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n","\n","    return full_model, encoder_model, decoder_model\n","\n","def train(full_model, enc_seq, dec_seq, batch_size, n_epochs=10):\n","    \"\"\" Training the model \"\"\"\n","    loss_epoch = []\n","    accuracy_epoch = []\n","    for ep in range(n_epochs):\n","        losses = []\n","        accuracies = []\n","        for bi in range(0, enc_seq.shape[0] - batch_size, batch_size):\n","\n","            enc_onehot_seq = to_categorical(\n","                enc_seq[bi:bi + batch_size, :], num_classes=enc_vsize)\n","            dec_onehot_seq = to_categorical(\n","                dec_seq[bi:bi + batch_size, :], num_classes=dec_vsize)\n","\n","            full_model.train_on_batch(\n","                [enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :])\n","\n","            l,a = full_model.evaluate([enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :],\n","                                    batch_size=batch_size, verbose=0)\n","\n","            losses.append(l)\n","            accuracies.append(a)\n","        if (ep + 1) % 1 == 0:\n","            print(\"Loss/Accuracy in epoch {}: {}/{}\".format(ep + 1, np.mean(losses), np.mean(accuracies)))\n","            loss_epoch.append(np.mean(losses))\n","            accuracy_epoch.append(np.mean(accuracies))\n","    return loss_epoch, accuracy_epoch\n","\n","\n","def infer_nmt(encoder_model, decoder_model, test_enc_seq, enc_vsize, dec_vsize, dec_timesteps):\n","    \"\"\"\n","    Infer logic\n","    :param encoder_model: keras.Model\n","    :param decoder_model: keras.Model\n","    :param test_en_seq: sequence of word ids\n","    :param en_vsize: int\n","    :param fr_vsize: int\n","    :return:\n","    \"\"\"\n","\n","    test_dec_seq = sents2sequences(dec_tokenizer, ['_'], dec_vsize)\n","    test_enc_onehot_seq = to_categorical(test_enc_seq, num_classes=enc_vsize)\n","    test_dec_onehot_seq = np.expand_dims(\n","        to_categorical(test_dec_seq, num_classes=dec_vsize), 1)\n","\n","    enc_outs, enc_last_state = encoder_model.predict(test_enc_onehot_seq)\n","    dec_state = enc_last_state\n","    attention_weights = []\n","    dec_text = ''\n","    for i in range(dec_timesteps):\n","\n","        dec_out, attention, dec_state = decoder_model.predict(\n","            [enc_outs, dec_state, test_dec_onehot_seq])\n","        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n","\n","        if dec_ind == 0:\n","            break\n","        test_dec_seq = sents2sequences(\n","            dec_tokenizer, [dec_index2word[dec_ind]], dec_vsize)\n","        test_dec_onehot_seq = np.expand_dims(\n","            to_categorical(test_dec_seq, num_classes=dec_vsize), 1)\n","\n","        attention_weights.append((dec_ind, attention))\n","        dec_text += dec_index2word[dec_ind]\n","\n","    return dec_text, attention_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KcOHyl-IP4N"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.rcParams['font.sans-serif']=[\"PingFang HK\"]\n","def plot_attention_weights(encoder_inputs, attention_weights, enc_id2word, dec_id2word, filename=None):\n","    \"\"\"\n","    Plots attention weights\n","    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n","    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n","    :param en_id2word: dict\n","    :param fr_id2word: dict\n","    :return:\n","    \"\"\"\n","\n","    if len(attention_weights) == 0:\n","        print('Your attention weights was empty. No attention map saved to the disk. ' +\n","              '\\nPlease check if the decoder produced  a proper translation')\n","        return\n","\n","    mats = []\n","    dec_inputs = []\n","    for dec_ind, attn in attention_weights:\n","        mats.append(attn.reshape(-1))\n","        dec_inputs.append(dec_ind)\n","    attention_mat = np.transpose(np.array(mats))\n","\n","    fig, ax = plt.subplots(figsize=(32, 32))\n","    ax.imshow(attention_mat)\n","\n","    ax.set_xticks(np.arange(attention_mat.shape[1]))\n","    ax.set_yticks(np.arange(attention_mat.shape[0]))\n","\n","    ax.set_xticklabels([dec_id2word[inp] if inp != 0 else \"<Res>\" for inp in dec_inputs])\n","    ax.set_yticklabels([enc_id2word[inp] if inp != 0 else \"<Res>\" for inp in encoder_inputs.ravel()])\n","\n","    ax.tick_params(labelsize=32)\n","    ax.tick_params(axis='x', labelrotation=90)\n","\n","#     if not os.path.exists(config.RESULTS_DIR):\n","#         os.mkdir(config.RESULTS_DIR)\n","#     if filename is None:\n","#         plt.savefig( 'attention.png'))\n","#     else:\n","#         plt.savefig(os.path.join(config.RESULTS_DIR, '{}'.format(filename)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLc_EcJFIP6j"},"outputs":[],"source":["#### hyperparameters\n","batch_size = 128\n","hidden_size = 256\n","n_epochs = 60\n","\n","# ### Getting sequence integer data\n","enc_seq = tr_enc_text\n","dec_seq = tr_dec_text\n","# ### timestesps\n","enc_timesteps = enc_seq.shape[1]\n","dec_timesteps = dec_seq.shape[1]\n","\n","# ### vocab size\n","answers_with_tags = list()\n","for i in range( len(answers) ):\n","    if type(answers[i]) == str:\n","        answers_with_tags.append( answers[i] )\n","    else:\n","        questions.pop(i)\n","\n","answers = list()\n","for i in range( len(answers_with_tags) ) :\n","    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n","\n","tokenizer = preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts( questions + answers )\n","enc_vsize = len( tokenizer.word_index )+1\n","dec_vsize = len( tokenizer.word_index )+1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGqNouheIP9L","outputId":"593587a8-a2f3-4df8-e268-c20dbfe383e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["3937\n","[[  1  28   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  1   7   8  42   2   0   0   0   0   0   0   0   0   0   0   0]\n"," [  1 272 273   2   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  1  20   6   2   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  1  49   2   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n","[[  1  34   9  35  21  13  29  27   7   2   0   0   0   0   0   0   0   0\n","    0   0   0   0]\n"," [  1  33  21  13  30   8  32  38  31  39   2   0   0   0   0   0   0   0\n","    0   0   0   0]\n"," [  1   8  19  15   4  17  16  14  18  20 233   2   0   0   0   0   0   0\n","    0   0   0   0]\n"," [  1  28   5  37  36   2   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0]\n"," [  1  34   9  35  21  13  29  27   7   2   0   0   0   0   0   0   0   0\n","    0   0   0   0]]\n"]}],"source":["print(enc_vsize)\n","print(tr_enc_text[:5])\n","print(tr_dec_text[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEGAO5uheK4m"},"outputs":[],"source":["import re\n","import keras\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense, GRU\n","from tensorflow.keras.layers import AdditiveAttention, Attention\n","import numpy as np\n","from random import randint\n","from numpy import array\n","from numpy import argmax\n","from numpy import array_equal\n","from keras import Model\n","from keras.models import Sequential\n","from keras.layers import LSTM, GRU, Concatenate\n","from keras.layers import Attention\n","from keras.layers import Dense\n","from keras.layers import TimeDistributed\n","from keras.layers import RepeatVector\n","from keras import Input\n","from tensorflow.keras.layers import Attention\n","\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhqbyHfIIP_w","outputId":"4500d8c5-f368-4a95-9021-19a4538bc762"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(128, 16, 3937)]    0           []                               \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(128, 21, 3937)]    0           []                               \n","                                                                                                  \n"," encoder_gru (GRU)              [(128, 16, 256),     3221760     ['encoder_inputs[0][0]']         \n","                                 (128, 256)]                                                      \n","                                                                                                  \n"," decoder_gru (GRU)              [(128, 21, 256),     3221760     ['decoder_inputs[0][0]',         \n","                                 (128, 256)]                      'encoder_gru[0][1]']            \n","                                                                                                  \n"," attention_layer (AdditiveAtten  ((128, 21, 256),    256         ['decoder_gru[0][0]',            \n"," tion)                           (128, 21, 16))                   'encoder_gru[0][0]']            \n","                                                                                                  \n"," concat_layer (Concatenate)     (128, 21, 512)       0           ['decoder_gru[0][0]',            \n","                                                                  'attention_layer[0][0]']        \n","                                                                                                  \n"," time_distributed_layer (TimeDi  (128, 21, 3937)     2019681     ['concat_layer[0][0]']           \n"," stributed)                                                                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,463,457\n","Trainable params: 8,463,457\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["full_model, infer_enc_model, infer_dec_model = define_nmt(\n","    hidden_size=hidden_size,\n","    batch_size=batch_size,\n","    enc_timesteps=enc_timesteps,\n","    dec_timesteps=dec_timesteps,\n","    enc_vsize=enc_vsize,\n","    dec_vsize=dec_vsize)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NgeJoTLYjbvA","outputId":"8f05f394-9891-457c-9db6-4d71c3f65dd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss/Accuracy in epoch 1: 1.5865275750215027/0.6939633786678314\n","Loss/Accuracy in epoch 2: 0.5226491306876314/0.9058854607329971\n","Loss/Accuracy in epoch 3: 0.35861094148251516/0.933714551658466\n","Loss/Accuracy in epoch 4: 0.30132842020013895/0.9428024086458929\n","Loss/Accuracy in epoch 5: 0.2684311082736514/0.9469876748391952\n","Loss/Accuracy in epoch 6: 0.25113995259777566/0.9487825825639155\n","Loss/Accuracy in epoch 7: 0.23746412984984017/0.9499403467808647\n","Loss/Accuracy in epoch 8: 0.22437586157914552/0.9515096916892063\n","Loss/Accuracy in epoch 9: 0.2106118811262322/0.9535772000578628\n","Loss/Accuracy in epoch 10: 0.19745648234290467/0.9553646244879427\n","Loss/Accuracy in epoch 11: 0.18505758310309559/0.956697710800445\n","Loss/Accuracy in epoch 12: 0.1732623667935252/0.958170838225847\n","Loss/Accuracy in epoch 13: 0.16137720668426148/0.9597583539869593\n","Loss/Accuracy in epoch 14: 0.15059303952382352/0.9615094307167776\n","Loss/Accuracy in epoch 15: 0.14046399276061305/0.9632893728798834\n","Loss/Accuracy in epoch 16: 0.1316379913804322/0.9649078911063315\n","Loss/Accuracy in epoch 17: 0.1247421235112733/0.9661704200780254\n","Loss/Accuracy in epoch 18: 0.11639261054762923/0.9678669763707567\n","Loss/Accuracy in epoch 19: 0.10791411989849548/0.9698393444220225\n","Loss/Accuracy in epoch 20: 0.10006785039350406/0.9715786632792703\n","Loss/Accuracy in epoch 21: 0.09326163780522244/0.9731747295664644\n","Loss/Accuracy in epoch 22: 0.0874888507405767/0.9745163670901594\n","Loss/Accuracy in epoch 23: 0.08207670642225602/0.9757275836563658\n","Loss/Accuracy in epoch 24: 0.07740713817658888/0.9770189742932375\n","Loss/Accuracy in epoch 25: 0.07315394678855604/0.9780302837319758\n","Loss/Accuracy in epoch 26: 0.06950065026166126/0.9788267120890234\n","Loss/Accuracy in epoch 27: 0.06589209692985563/0.9796552152811796\n","Loss/Accuracy in epoch 28: 0.06287662045101518/0.9803233591989539\n","Loss/Accuracy in epoch 29: 0.06001483875155535/0.9809113283609522\n","Loss/Accuracy in epoch 30: 0.057922441583767734/0.9814725710057665\n","Loss/Accuracy in epoch 31: 0.05625718316510213/0.9818542171483753\n","Loss/Accuracy in epoch 32: 0.05528556452374959/0.9819728805758487\n","Loss/Accuracy in epoch 33: 0.05434977901673303/0.982201652622771\n","Loss/Accuracy in epoch 34: 0.053235739926240894/0.9824528754100033\n","Loss/Accuracy in epoch 35: 0.05237787126847168/0.9826153679483238\n","Loss/Accuracy in epoch 36: 0.051722216350628576/0.98268913195051\n","Loss/Accuracy in epoch 37: 0.050422988559692114/0.9829606675896151\n","Loss/Accuracy in epoch 38: 0.049405028770079615/0.9831520233346128\n","Loss/Accuracy in epoch 39: 0.048767567251778166/0.9831873017481003\n","Loss/Accuracy in epoch 40: 0.04777555662465226/0.9833668991066944\n","Loss/Accuracy in epoch 41: 0.047060868705755994/0.9833904177635565\n","Loss/Accuracy in epoch 42: 0.04645053941094793/0.9835197718321592\n","Loss/Accuracy in epoch 43: 0.04568566006416624/0.9837089914014969\n","Loss/Accuracy in epoch 44: 0.045323540059760636/0.9838447591354107\n","Loss/Accuracy in epoch 45: 0.04518318165698485/0.983868275908218\n","Loss/Accuracy in epoch 46: 0.044921686491897546/0.9839933535148357\n","Loss/Accuracy in epoch 47: 0.044674565972447206/0.9839965606900467\n","Loss/Accuracy in epoch 48: 0.0444479046625971/0.9840735320387215\n","Loss/Accuracy in epoch 49: 0.04424671009708137/0.984152639734334\n","Loss/Accuracy in epoch 50: 0.044201475027185896/0.9840713924375074\n","Loss/Accuracy in epoch 51: 0.04421153989247175/0.9840756695846031\n","Loss/Accuracy in epoch 52: 0.0441850586528368/0.9841387417467161\n","Loss/Accuracy in epoch 53: 0.0440954732116186/0.9841205681535019\n","Loss/Accuracy in epoch 54: 0.043861098505553905/0.984214642609673\n","Loss/Accuracy in epoch 55: 0.04392946832084054/0.9841088101096537\n","Loss/Accuracy in epoch 56: 0.04379106460817576/0.9840767385288217\n","Loss/Accuracy in epoch 57: 0.04399279911648455/0.9839922842280618\n","Loss/Accuracy in epoch 58: 0.043912817782424014/0.9840158033987572\n","Loss/Accuracy in epoch 59: 0.04385319291901689/0.9840104578212759\n","Loss/Accuracy in epoch 60: 0.04377962605998953/0.9840147349683718\n"]}],"source":["loss, accuracy = train(full_model, enc_seq, dec_seq, ,batch_size n_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5MXU0sMjkHS"},"outputs":[],"source":[]},{"cell_type":"code","source":["import pickle # save model\n","path = '/content/drive/My Drive/ColabNotebooks/298B_Data_Add_New/'\n","\n","pickle.dump(full_model, open(path + 'Bahdanau_Attention_Ting_60.pkl', 'wb')) "],"metadata":{"id":"llilsGq0Kiw0"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
